{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec2e318-c793-49da-abf6-9b4fe4072e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import code\n",
    "import json\n",
    "import scipy\n",
    "import difflib\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Point\n",
    "from shapely.ops import unary_union\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "\n",
    "#--- Unity\n",
    "pous_data_dir = \"/POUS\"\n",
    "stormdata_dir = \"/USA_storms_data\"\n",
    "counties_shapefile = \"cb_2018_us_county_500k.shp\"\n",
    "eia_data_dir = \"/EIA_reliability_2016_2021\"\n",
    "output_dir = \"/POUS_and_storms\"\n",
    "\n",
    "#----\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e83bf9-6dd2-417e-8302-15ce7cede266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = np.load(file=open(\"/gypsum/eguide/projects/zshah/data/NL_OutageDetection/training_data/USA/outline/USA_48201_outline.npz\",\"rb\"))[\"outline\"]\n",
    "# plt.imshow(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fef8e0-da6d-4d51-97b8-282b9f92e41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load extra data\n",
    "\"\"\"\n",
    "# load mapping of county names and FIPS to states\n",
    "county2fips = pd.read_csv(\"/gypsum/eguide/projects/zshah/data/USA_fips2county/fips2county.tsv\", delimiter=\"\\t\")\n",
    "county2fips = county2fips[[\"CountyFIPS\",\"CountyName\",\"StateName\"]].drop_duplicates()\n",
    "county2fips[\"StateName\"] = county2fips[\"StateName\"].str.replace(r'[^A-Za-z]+', '', regex=True).str.lower()\n",
    "county2fips[\"CountyName\"] = county2fips[\"CountyName\"].str.replace(r'[^A-Za-z]+', '', regex=True).str.lower()\n",
    "county2fips[\"CountyFIPS\"] = county2fips[\"CountyFIPS\"].apply(lambda x: str(x).zfill(5))\n",
    "\n",
    "# dictionary of state to list of county names\n",
    "state2county = county2fips.groupby([\"StateName\"])\n",
    "state2county = {key: group[\"CountyName\"].tolist() for key, group in state2county}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26146de0-21db-45b8-a0e3-77cbb8a854e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--*--\"*30, flush=True)\n",
    "print(\"Loading and processing storms data.\", flush=True)\n",
    "#\n",
    "\n",
    "\"\"\"\n",
    "STEP 0: use county shapefile and NWPS Zones file to map zone numbers to countyFIPS\n",
    "Reason: some entries in the storm dataset have zone numbers whereas some entries have countyFIPS. For consistency, we convert everything to countyFIPS.\n",
    "\"\"\"\n",
    "# process zones dataframe (updated z_22mr22.shp to z_08mr23.shp)\n",
    "crs = \"EPSG:4326\" # NOTE: works only in usa_env conda environment. Newer versions of geopandas might raise an error\n",
    "zones = gpd.read_file(stormdata_dir + \"/NWPS_zones/z_08mr23.shp\")[[\"STATE\",\"STATE_ZONE\",\"geometry\"]]\n",
    "zones = zones.to_crs(crs)\n",
    "\n",
    "# sjoin with counties data (one zone can intersect with multiple counties)\n",
    "counties = gpd.read_file(counties_shapefile)[[\"STATEFP\",\"COUNTYFP\",\"geometry\"]]\n",
    "counties = counties.to_crs(crs)\n",
    "counties.loc[:,\"CountyFIPS\"] = counties.loc[:,\"STATEFP\"] + counties.loc[:,\"COUNTYFP\"]\n",
    "counties[\"county_geom\"] = counties[\"geometry\"]\n",
    "\n",
    "# Perform spatial join using 'intersects'\n",
    "z2c = gpd.sjoin(zones, counties, op=\"intersects\", how=\"inner\")\n",
    "\n",
    "# Calculate the area of the intersection between each zone and county\n",
    "z2c[\"intersection_area\"] = z2c.geometry.intersection(z2c[\"county_geom\"]).area\n",
    "\n",
    "# Filter out matches with minimal overlap/intersection (threshold was manually selected looking at Los Angeles county and the corresponding zones)\n",
    "z2c = z2c[z2c.intersection_area >= 1e-3]\n",
    "\n",
    "# Add zones that lie within a county but might be eliminated due to small self-area\n",
    "z2c_within = gpd.sjoin(zones, counties, op=\"within\", how=\"inner\")\n",
    "\n",
    "# combine the two dataframes\n",
    "z2c = pd.concat([z2c, z2c_within]).reset_index(drop=True)\n",
    "\n",
    "# find direct mapping of state shortname to state fips.\n",
    "# since some zones intersect multiple states, stateFP and state_zone mapping needs to be cleaned first\n",
    "state2fips = z2c[[\"STATE\",\"STATEFP\"]].drop_duplicates()\n",
    "state2fips = state2fips.groupby(\"STATE\")[\"STATEFP\"].apply(lambda x: x.mode()[0])\n",
    "\n",
    "# merge this with the z2c dataframe\n",
    "z2c = z2c[[\"STATE\",\"STATE_ZONE\",\"CountyFIPS\"]].drop_duplicates()\n",
    "z2c = pd.merge(z2c, state2fips, on=[\"STATE\"], how=\"left\")\n",
    "\n",
    "# reformat STATEZONE column from: (statename + zone number) format ---> (statefips + zone number) format\n",
    "# this is to ensure compatibility with the NOAA storms database\n",
    "z2c[\"ZoneFIPS\"] = z2c.apply(lambda x: x[\"STATEFP\"] + x[\"STATE_ZONE\"][2:], axis=1)\n",
    "z2c[\"CZ_TYPE\"] = \"Z\"\n",
    "\n",
    "display(z2c)\n",
    "    \n",
    "# z2c[\"State\"] = z2c[\"STATE_ZONE\"].apply(lambda x: x[0:2])\n",
    "# # NV gets 04 (AZ) STATEFP instead of 32; MO Missouri gets 05 (AR) instead of 29 in some cases. Centroids of these confused souls lie in an out-of-state county. We ignore these points for simplicity.\n",
    "# condition1 = ((z2c[\"State\"]==\"NV\") & (z2c[\"STATEFP\"]==\"04\"))\n",
    "# condition2 = ((z2c[\"State\"]==\"MO\") & (z2c[\"STATEFP\"]==\"05\"))\n",
    "# z2c = z2c[(~condition1) & (~condition2)].reset_index(drop=True)\n",
    "# #\n",
    "# \"\"\"\n",
    "# STEP 1: merge all the stormevents csvs [cols_of_interest]\n",
    "# \"\"\"\n",
    "# all_files = glob.glob(stormdata_dir + \"/*.csv\")\n",
    "# ds = []\n",
    "# for file in all_files:\n",
    "#     temp = pd.read_csv(file)\n",
    "#     ds.append(temp)\n",
    "#     del(temp)\n",
    "# ds = pd.concat(ds)\n",
    "# ds.loc[:,\"STATE\"] = ds[\"STATE\"].str.replace(r'\\W', '', regex=True).str.lower()\n",
    "# #\n",
    "# \"\"\"\n",
    "# STEP 2: assign countyFIPS to all entries in CZ_TYPE == \"z\" in the storms dataframe.\n",
    "# NOTE: we use both, CZ_TYPE == \"c\" and \"z\" in this work, where \"c\" represents county and \"z\" represents zone.\n",
    "# \"\"\"\n",
    "# # create a CountyFIPS column in storms data using CZ_FIPS and STATE_FIPS\n",
    "# ds[[\"CZ_FIPS\",\"STATE_FIPS\"]] = ds[[\"CZ_FIPS\",\"STATE_FIPS\"]].astype(str)\n",
    "# ds.loc[:,\"CZ_FIPS\"] = ds[\"CZ_FIPS\"].apply(lambda x: x.zfill(3))\n",
    "# ds.loc[:,\"CZ_FIPS\"] = (ds[\"STATE_FIPS\"] + ds[\"CZ_FIPS\"]).astype(int)\n",
    "# ds = pd.merge(ds, z2c[[\"CountyFIPS\",\"ZoneFIPS\"]], left_on=\"CZ_FIPS\", right_on=\"ZoneFIPS\", how=\"left\")\n",
    "# ds[\"CountyFIPS\"] = ds.apply(lambda x: x[\"CZ_FIPS\"] if x[\"CZ_TYPE\"]==\"C\" else x[\"CountyFIPS\"] if x[\"CZ_TYPE\"]==\"Z\" else np.nan, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb02b21-3b11-4107-9a23-cc30cfc17c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load storm data and add the mapping of zone to county\n",
    "\"\"\"\n",
    "cols_of_interest = [\"BEGIN_DATE_TIME\",\"END_DATE_TIME\",\"CZ_TIMEZONE\",\"EPISODE_ID\",\"EVENT_ID\",\"STATE_FIPS\",\"CZ_FIPS\",\"STATE\",\"CZ_NAME\",\"CZ_TYPE\",\"EVENT_TYPE\",\"BEGIN_LAT\",\"BEGIN_LON\",\"END_LAT\",\"END_LON\"]\n",
    "all_files = glob.glob(stormdata_dir + \"/*_c2022*.csv\") #use the most updated files (they were updated in 2023). \n",
    "ds = []\n",
    "for file in all_files:\n",
    "    temp = pd.read_csv(file)[cols_of_interest]\n",
    "    ds.append(temp)\n",
    "    print(\"File = {} | rows = {}\".format(file, len(temp)))\n",
    "    # display(temp.head())\n",
    "    del(temp)\n",
    "ds = pd.concat(ds)\n",
    "ds.loc[:,\"STATE\"] = ds[\"STATE\"].str.replace(r'\\W', '', regex=True).str.lower()\n",
    "\n",
    "print(\"total episodes = {} | total events = {}\".format(ds[\"EPISODE_ID\"].nunique(), ds[\"EVENT_ID\"].nunique()))\n",
    "\n",
    "ds.loc[:,\"CZ_FIPS\"] = ds.apply(lambda x: str(x[\"STATE_FIPS\"]).zfill(2) + str(x[\"CZ_FIPS\"]).zfill(3), axis=1)\n",
    "\n",
    "# perform the join only for \"Zones\"\n",
    "ds = pd.merge(ds, z2c[[\"CountyFIPS\",\"ZoneFIPS\",\"CZ_TYPE\"]], left_on=[\"CZ_TYPE\",\"CZ_FIPS\"], right_on=[\"CZ_TYPE\",\"ZoneFIPS\"], how=\"left\")\n",
    "\n",
    "display(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ff553a-d9e0-40ad-aeda-81e85399869d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wherever the dataset states \"C\", the CZ_FIPS column gives us county FIPS --> leave that as is\n",
    "# wherever the dataset states \"Z\", the CZ_FIPS gives the zone FIPS --> use mapping of zone FIPS to CountyFIPS\n",
    "ds[\"CountyFIPS\"] = ds.apply(lambda x: x[\"CZ_FIPS\"] if x[\"CZ_TYPE\"]==\"C\" else x[\"CountyFIPS\"] if x[\"CZ_TYPE\"]==\"Z\" else np.nan, axis=1)\n",
    "\n",
    "# check if any of the \"zones\" have CZ_FIPS same as county FIPS\n",
    "countyfips = counties[\"CountyFIPS\"].unique()\n",
    "ds[\"CountyFIPS\"] = ds.apply(lambda x: x[\"CZ_FIPS\"] if ((x[\"CZ_TYPE\"]==\"Z\") & (pd.isnull(x[\"CountyFIPS\"]) & (x[\"CZ_FIPS\"] in countyfips))) else x[\"CountyFIPS\"], axis=1)\n",
    "\n",
    "# many of the rows with missing county FIPS correspond to marine zones, which are not covered in the NWPS zones\n",
    "# since marine zones are limited to the beach/coastal areas, we would hardly find overlap between a county and marine zone\n",
    "# so instead we will use BEGIN_LAT/LON and END_LAT/LON\n",
    "ds_missing_CountyFIPS = ds[(pd.isnull(ds[\"CountyFIPS\"]))].copy().drop(columns=[\"CountyFIPS\"])\n",
    "\n",
    "ds = (\n",
    "    ds\n",
    "    .dropna(subset=[\"CountyFIPS\"])\n",
    "    .drop(columns=[\"ZoneFIPS\",\"CZ_TYPE\",\"CZ_FIPS\",\"CZ_NAME\",\"STATE_FIPS\",\"STATE\",\"BEGIN_LAT\",\"BEGIN_LON\",\"END_LAT\",\"END_LON\"])\n",
    ")\n",
    "\n",
    "print(\"total episodes = {} | total events = {}\".format(ds[\"EPISODE_ID\"].nunique(), ds[\"EVENT_ID\"].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f496094a-ea38-4e54-b53c-dd64a5e08d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_missing_CountyFIPS[\"begin_geometry\"] = ds_missing_CountyFIPS.apply(lambda x: Point(x[\"BEGIN_LON\"], x[\"BEGIN_LAT\"]), axis=1)\n",
    "ds_missing_CountyFIPS[\"end_geometry\"] = ds_missing_CountyFIPS.apply(lambda x: Point(x[\"END_LON\"], x[\"END_LAT\"]), axis=1)\n",
    "ds_missing_CountyFIPS = ds_missing_CountyFIPS.drop(columns = [\"BEGIN_LAT\",\"BEGIN_LON\",\"END_LAT\",\"END_LON\"])\n",
    "\n",
    "#\n",
    "ds_missing_CountyFIPS_begin = ds_missing_CountyFIPS.drop(columns=[\"end_geometry\"]).rename(columns={\"begin_geometry\":\"geometry\"})\n",
    "ds_missing_CountyFIPS_end = ds_missing_CountyFIPS.drop(columns=[\"begin_geometry\"]).rename(columns={\"end_geometry\":\"geometry\"})\n",
    "\n",
    "#\n",
    "ds_missing_CountyFIPS = pd.concat([ds_missing_CountyFIPS_begin, ds_missing_CountyFIPS_end]).reset_index(drop=True)\n",
    "ds_missing_CountyFIPS = gpd.GeoDataFrame(ds_missing_CountyFIPS, geometry=\"geometry\", crs=\"EPSG:4269\")\n",
    "ds_missing_CountyFIPS = ds_missing_CountyFIPS.to_crs(\"EPSG:4326\")\n",
    "ds_missing_CountyFIPS = gpd.sjoin(ds_missing_CountyFIPS, counties.drop(columns=[\"county_geom\",\"STATEFP\",\"COUNTYFP\"]), op=\"within\", how=\"left\")\n",
    "\n",
    "#\n",
    "ds_other = ds_missing_CountyFIPS[(pd.isnull(ds_missing_CountyFIPS[\"CountyFIPS\"]))].drop(columns=[\"index_right\",\"CountyFIPS\"]).reset_index(drop=True)\n",
    "ds_missing_CountyFIPS = ds_missing_CountyFIPS[(~pd.isnull(ds_missing_CountyFIPS[\"CountyFIPS\"]))].drop(columns=[\"STATE_FIPS\",\"STATE\",\"geometry\",\"index_right\",\"ZoneFIPS\",\"CZ_TYPE\",\"CZ_FIPS\",\"CZ_NAME\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32643c07-ef1c-46b7-8abd-ed86b9cccfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "What is ds_other?\n",
    "\n",
    "- it contains points (lon, lat) that do not belong to any county or forecast zone\n",
    "- it contains points with empty lat, lon, and whose CZ_FIPS don't seem to be associated with any zone or counties. It almost seems like the FIPS are wrong.\n",
    "    - we cannot do much in this case. If the CZ_NAME contains a county name we use it, else we discard the rows\n",
    "\"\"\"\n",
    "ds_other_coords = ds_other[(pd.notnull(ds_other[\"geometry\"].x) & pd.notnull(ds_other[\"geometry\"].y))]\n",
    "ds_other_nocoords = ds_other[(pd.isnull(ds_other[\"geometry\"].x) | pd.isnull(ds_other[\"geometry\"].y))]\n",
    "\n",
    "# ds_other = gpd.sjoin_nearest(ds_marine, counties.drop(columns=[\"county_geom\",\"STATEFP\",\"COUNTYFP\"]), how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad81395f-b961-4576-91e1-a410b1442c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal with ds_other_nocoords by using county name-based regex expressions\n",
    "ds_other_nocoords[\"counties\"] = ds_other_nocoords[\"STATE\"].apply(lambda x: state2county[x] if x in state2county else np.nan)\n",
    "\n",
    "# drop rows with states like atlanticsouth or gulfofmexico\n",
    "ds_other_nocoords = ds_other_nocoords.dropna(subset=[\"counties\"])\n",
    "\n",
    "# obtain the actual county names from the CZ_NAME statements. \n",
    "ds_other_nocoords[\"CZ_NAME_regex\"] = ds_other_nocoords[\"CZ_NAME\"].str.replace(r'[^A-Za-z]+', '', regex=True).str.lower()\n",
    "ds_other_nocoords[\"CountyName\"] = ds_other_nocoords.apply(lambda x: [y for y in x[\"counties\"] if y in x[\"CZ_NAME_regex\"]], axis=1)\n",
    "\n",
    "# some events would be spread across multiple counties --> we find multiple matches --> we explode each county it into an individual row.\n",
    "ds_other_nocoords = ds_other_nocoords.explode(column=\"CountyName\").reset_index(drop=True)\n",
    "\n",
    "# we will be left with entries where no match was found, and so we eliminate those rows\n",
    "ds_other_nocoords = ds_other_nocoords[(~pd.isnull(ds_other_nocoords[\"CountyName\"]))]\n",
    "\n",
    "# add countyfips corresponding to that county\n",
    "ds_other_nocoords = pd.merge(ds_other_nocoords, county2fips, left_on=[\"STATE\",\"CountyName\"], right_on=[\"StateName\",\"CountyName\"], how=\"left\")\n",
    "ds_other_nocoords = ds_other_nocoords.dropna(subset=[\"CountyFIPS\"]).reset_index(drop=True)\n",
    "\n",
    "# only extract the columns of interest\n",
    "ds_other_nocoords = ds_other_nocoords[[\"BEGIN_DATE_TIME\",\"END_DATE_TIME\",\"CZ_TIMEZONE\",\"EPISODE_ID\",\"EVENT_ID\",\"EVENT_TYPE\",\"CountyFIPS\"]].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "display(ds_other_nocoords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d7c31d-92a0-4110-9d26-05fd6b63db93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Deal with ds_other_coords by finding the closest county to the lat lon of the event\n",
    "\n",
    "def find_nearest_county(points_df, polygons_df):\n",
    "    # Build the spatial index for polygons_df\n",
    "    polygons_index = polygons_df.sindex\n",
    "\n",
    "    # Create an empty list to store the nearest polygon ID for each point\n",
    "    nearest_polygon_ids = []\n",
    "\n",
    "    # Iterate over each point in points_df\n",
    "    for index, point in points_df.iterrows():\n",
    "        # Get the bounds of the point geometry\n",
    "        point_bounds = point.geometry.bounds\n",
    "\n",
    "        # Find the nearest polygon using the spatial index\n",
    "        nearest_index = polygons_index.nearest(point_bounds)\n",
    "\n",
    "        # Get the ID of the nearest polygon\n",
    "        nearest_polygon_id = polygons_df.iloc[nearest_index][\"CountyFIPS\"].values\n",
    "        # print(nearest_polygon_id)\n",
    "\n",
    "        # Add the nearest polygon ID to the list\n",
    "        nearest_polygon_ids.append(nearest_polygon_id)\n",
    "\n",
    "    # # Add the nearest polygon ID column to points_df\n",
    "    points_df[\"CountyFIPS\"] = nearest_polygon_ids\n",
    "    points_df = points_df.explode(column=\"CountyFIPS\").reset_index(drop=True)\n",
    "    return points_df\n",
    "\n",
    "    \n",
    "ds_other_coords = find_nearest_county(points_df=ds_other_coords.copy(), polygons_df=counties[[\"CountyFIPS\",\"geometry\"]].drop_duplicates().reset_index(drop=True))\n",
    "\n",
    "# only extract the columns of interest\n",
    "ds_other_coords = ds_other_coords[[\"BEGIN_DATE_TIME\",\"END_DATE_TIME\",\"CZ_TIMEZONE\",\"EPISODE_ID\",\"EVENT_ID\",\"EVENT_TYPE\",\"CountyFIPS\"]].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "display(ds_other_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f066adc9-28a4-4ed3-b633-fa561eb8a840",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(ds)\n",
    "\n",
    "display(ds_missing_CountyFIPS)\n",
    "\n",
    "display(ds_other_coords)\n",
    "\n",
    "display(ds_other_nocoords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732196d7-ef07-485e-8694-3de7359514bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.concat([ds, ds_missing_CountyFIPS, ds_other_coords, ds_other_nocoords]).reset_index(drop=True)\n",
    "\n",
    "print(\"total episodes = {} | total events = {}\".format(ds[\"EPISODE_ID\"].nunique(), ds[\"EVENT_ID\"].nunique()))\n",
    "\n",
    "display(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d97de46-dd44-4af0-88c3-66403930559e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "STEP 3: \n",
    "- convert CZ_TIMEZONE to native Python format. \n",
    "- convert local ts to UTC (POUS is in UTC).\n",
    "\"\"\"\n",
    "timezones = {\"EST-5\":\"US/Eastern\",\"EDT-4\":\"US/Eastern\",\"CST-6\":\"US/Central\",\"CDT-5\":\"US/Central\",\"MST-7\":\"US/Mountain\",\"PST-8\":\"US/Pacific\",\"PDT-7\":\"US/Pacific\",\"AKST-9\":\"US/Alaska\",\"HST-10\":\"US/Hawaii\"}\n",
    "ds.loc[:,\"tz\"] = ds[\"CZ_TIMEZONE\"].apply(lambda x: timezones[x] if x in timezones.keys() else np.nan)\n",
    "# we drop nans and remove virgin islands\n",
    "ds = ds.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a2dad8-6793-43b6-8da3-21147c66fd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "STEP 4: process string timestamps and convert tz to UTC\n",
    "\"\"\"\n",
    "ds.loc[:,\"BEGIN_DATE_TIME\"] = pd.to_datetime(ds[\"BEGIN_DATE_TIME\"], format='%d-%b-%y %H:%M:%S')\n",
    "ds.loc[:,\"END_DATE_TIME\"] = pd.to_datetime(ds[\"END_DATE_TIME\"], format='%d-%b-%y %H:%M:%S')\n",
    "\n",
    "# ds.loc[:,[\"BEGIN_DATE_TIME\",\"END_DATE_TIME\"]] = ds.loc[:,[\"BEGIN_DATE_TIME\",\"END_DATE_TIME\"]].apply(pd.to_datetime, format='%d-%b-%y %H:%M:%S')\n",
    "ds = ds.sort_values(by=[\"CountyFIPS\",\"BEGIN_DATE_TIME\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9a9fc9-5a36-40b5-877b-e8234bb0d145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds.groupby([\"CountyFIPS\"]).tz.nunique().reset_index().sort_values(by=\"tz\").tail(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f9f353-0185-4b8f-801f-4f2d57308c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2901a03f-ad39-45c0-8f1c-46de4f6c60d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterating using groupby because some countyFIPS have two tz. County-wise iteration also helps with nonexistent and ambigous timestamps due to DST.\n",
    "ds_utc = []\n",
    "for idx, grouped in ds.groupby([\"CountyFIPS\",\"tz\"]):\n",
    "    grouped.loc[:,\"BEGIN_DATE_TIME\"] = grouped.apply(lambda row: row[\"BEGIN_DATE_TIME\"].tz_localize(row['tz'], nonexistent='shift_forward').tz_convert('UTC').tz_localize(None), axis=1)\n",
    "    grouped.loc[:,\"END_DATE_TIME\"] = grouped.apply(lambda row: row[\"END_DATE_TIME\"].tz_localize(row['tz'], nonexistent='shift_forward', ambiguous='NaT').tz_convert('UTC').tz_localize(None), axis=1)\n",
    "    ds_utc.append(grouped)\n",
    "ds_utc = pd.concat(ds_utc).dropna().reset_index(drop=True) # we lose only 1 ambiguous reading here\n",
    "ds_utc = (\n",
    "    ds_utc[[\"BEGIN_DATE_TIME\",\"END_DATE_TIME\",\"EPISODE_ID\",\"EVENT_ID\",\"EVENT_TYPE\",\"CountyFIPS\"]]\n",
    "    .rename(columns={\"BEGIN_DATE_TIME\":\"event_start\", \"END_DATE_TIME\":\"event_end\", \"EVENT_ID\":\"event_id\",\"EVENT_TYPE\":\"event_type\",\"EPISODE_ID\":\"episode_id\"}) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e46ae1e-4e2f-44a2-83ad-77a1e4f3fbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_utc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a73ee23-933d-4101-923e-b58ca3375b7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For each (event_id, event_type, CountyFIPS) tuple, create one row per day of the event. \n",
    "\"\"\"\n",
    "db = []\n",
    "for _, row in ds_utc.iterrows():\n",
    "    event_start = row[\"event_start\"].date()\n",
    "    event_end = row[\"event_end\"].date()\n",
    "    # print(row.T)\n",
    "    #\n",
    "    date_range = pd.date_range(event_start, event_end, freq=\"1D\")\n",
    "    #\n",
    "    temp = pd.DataFrame({\n",
    "        \"episode_id\":row[\"episode_id\"],\n",
    "        \"event_id\":row[\"event_id\"],\n",
    "        \"event_type\":row[\"event_type\"],\n",
    "        \"CountyFIPS\":row[\"CountyFIPS\"],\n",
    "        \"event_date\":date_range,\n",
    "    })\n",
    "    db.append(temp)\n",
    "#\n",
    "db = pd.concat(db).reset_index(drop=True)\n",
    "\n",
    "display(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fef11c8-de18-48e0-a1a0-568e0af3dfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3616e541-a5e0-4479-a22e-4bd9f7a27d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the storms dataframe\n",
    "output_dir = \"/StormEvents_processed\"\n",
    "output_filename = \"StormEvents_c2022.pkl\" #2022 version\n",
    "\n",
    "db.to_pickle(os.path.join(output_dir, output_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf28b5a-8d2e-4f05-8322-41d104666798",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
